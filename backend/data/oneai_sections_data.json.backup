{
  "preproduction-script": {
    "id": "preproduction-script",
    "name": "Script",
    "group": "preproduction",
    "nodes": [
      {
        "id": "input-preproduction-script",
        "type": "input",
        "label": "Input",
        "position": {
          "x": 100.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedTo": [
          "worker-1749286886724"
        ],
        "connectedFrom": [],
        "code": "",
        "output": {
          "text": "Title : ì–´ë¦° ì†Œë…„ì˜ ëª¨í—˜\nStory : í•œ ì†Œë…€ì´ ìˆ²ì†ìœ¼ë¡œ ëª¨í—˜ì„ ë– ë‚˜ë ¤ í•˜ê³  ìˆë‹¤. ì˜¤ë¥¸ìª½ì—ëŠ” ì“°ëŸ¬ì ¸ê°€ëŠ” ì˜› ì„±ì˜ ë°˜ìª½ì´ ë‚¨ì•„ìˆê³  ì™¼ìª½ì—ëŠ” ìˆ²ì´ ë¬´ì„±í•˜ê³  ê°€ìš´ë°ì—ëŠ” ê¸¸ì´ í•˜ë‚˜ ë‚˜ ìˆë‹¤\nê°€ìš´ë°ì— ê¸¸ì´ í•˜ë‚˜ ë‚˜ ìˆê³ , ì†Œë…„ì´ ë“±ì—ëŠ” ì¹¼ê³¼ í—ˆë¦„í•œ ì˜·ì„ ì…ê³  ê·¸ ê¸¸ë¡œ ê°€ë ¤ê³  í•˜ê³  ìˆë‹¤.",
          "type": "script"
        },
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default",
        "projectId": "bb03d6f6-8bc0-4ea0-b56a-8b5e93ee5441"
      },
      {
        "id": "output-preproduction-script",
        "type": "output",
        "label": "Output",
        "position": {
          "x": 4973.185096318626,
          "y": 610.9477592130473
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [
          "worker-1749360498922"
        ],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      },
      {
        "id": "worker-1749286886724",
        "type": "worker",
        "label": "Premise Parser",
        "position": {
          "x": 370.19451016605535,
          "y": 629.6101105917462
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "tasks": [
          {
            "id": "task-1749286886724",
            "text": "Analyze raw story text to identify genre, theme, and narrative type",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749286897591",
            "text": "Extract fundamental elements (characters, locations, conflicts, motivations) without predefined categories",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749286932041",
            "text": "Create an initial story DNA that subsequent nodes can expand upon",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749359962226",
            "text": "Generate metadata about story complexity and special requirements",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          }
        ],
        "connectedTo": [
          "worker-1749357862698"
        ],
        "connectedFrom": [
          "input-preproduction-script"
        ],
        "code": "# ========================================================================\r\n# BASE CODE - ê³µí†µ ì‹¤í–‰ ì½”ë“œ (ìˆ˜ì • ë¶ˆê°€)\r\n# ì´ ì½”ë“œëŠ” ëª¨ë“  Worker ë…¸ë“œê°€ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ ì‹¤í–‰ ì½”ë“œì…ë‹ˆë‹¤.\r\n# ========================================================================\r\n\r\nimport json\r\nimport time\r\n\r\n# ì—°ê²°ëœ ì…ë ¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\r\ninput_data = get_connected_outputs()\r\nprint(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Connected inputs received\")\r\nprint(json.dumps(input_data, ensure_ascii=False, indent=2))\r\n\r\n# í˜„ì¬ ë…¸ë“œ ì •ë³´ ì¶œë ¥\r\nprint(f\"\\nNode: {current_node.get('label', 'Unknown')}\")\r\nprint(f\"Purpose: {node_purpose}\")\r\nprint(f\"Expected Output Format: {output_format_description}\")\r\n\r\n# AI ëª¨ë¸ ì„¤ì • í™•ì¸ - execution.pyì—ì„œ ë…¸ë“œ ì„¤ì •ìœ¼ë¡œë¶€í„° ì œê³µë¨\r\nprint(f\"\\n[DEBUG] model_name: {model_name}\")\r\nprint(f\"[DEBUG] lm_studio_url: {lm_studio_url}\")\r\nprint(f\"[DEBUG] current_node['model']: {current_node.get('model', 'Not found')}\")\r\nprint(f\"[DEBUG] current_node['lmStudioUrl']: {current_node.get('lmStudioUrl', 'Not found')}\")\r\n\r\nif model_name == 'none' or not model_name or not lm_studio_url:\r\n    print(\"\\nâš ï¸  No AI model configured!\")\r\n    output = {\r\n        \"error\": \"No AI model configured\",\r\n        \"hint\": \"Please connect to LM Studio and select a model\",\r\n        \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S')\r\n    }\r\nelse:\r\n    print(f\"\\nâœ… Using AI model: {model_name}\")\r\n    \r\n    # ========================================================================\r\n    # ì…ë ¥ ë°ì´í„° í†µí•©\r\n    # ========================================================================\r\n    combined_input = \"\"\r\n    for key, value in input_data.items():\r\n        if isinstance(value, dict):\r\n            if 'text' in value:\r\n                combined_input += f\"[{key}]\\n{value['text']}\\n\\n\"\r\n            elif 'content' in value:\r\n                combined_input += f\"[{key}]\\n{value['content']}\\n\\n\"\r\n            else:\r\n                combined_input += f\"[{key}]\\n{json.dumps(value, ensure_ascii=False, indent=2)}\\n\\n\"\r\n        elif isinstance(value, str):\r\n            combined_input += f\"[{key}]\\n{value}\\n\\n\"\r\n        else:\r\n            combined_input += f\"[{key}]\\n{str(value)}\\n\\n\"\r\n    \r\n    # ========================================================================\r\n    # AI í”„ë¡¬í”„íŠ¸ êµ¬ì„±\r\n    # ========================================================================\r\n    base_prompt = f\"\"\"ë‹¹ì‹ ì€ ë‹¤ìŒ ëª©ì ì„ ë‹¬ì„±í•´ì•¼ í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤:\r\n\r\n**ëª©ì  (Purpose):**\r\n{node_purpose}\r\n\r\n**ì…ë ¥ ë°ì´í„°:**\r\n{combined_input.strip()}\r\n\r\n**ìˆ˜í–‰í•  ì‘ì—…ë“¤ (Tasks):**\"\"\"\r\n    \r\n    # Tasks ì¶”ê°€\r\n    if 'tasks' in current_node and current_node['tasks']:\r\n        for i, task in enumerate(current_node['tasks'], 1):\r\n            base_prompt += f\"\\n{i}. {task['text']}\"\r\n            # Task statusì— ë”°ë¥¸ ì¶”ê°€ ì§€ì‹œ\r\n            if task.get('taskStatus') == 'locked':\r\n                base_prompt += \" [í•„ìˆ˜ - ë°˜ë“œì‹œ ìˆ˜í–‰]\"\r\n            elif task.get('taskStatus') == 'low_priority':\r\n                base_prompt += \" [ì„ íƒì  - ê°€ëŠ¥í•œ ê²½ìš° ìˆ˜í–‰]\"\r\n    else:\r\n        base_prompt += \"\\n(ì‘ì—…ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)\"\r\n    \r\n    base_prompt += f\"\"\"\\n\\n**ê¸°ëŒ€í•˜ëŠ” ì¶œë ¥ í˜•ì‹:**\r\n{output_format_description}\r\n\r\nìœ„ì˜ ëª©ì ê³¼ ì‘ì—…ë“¤ì„ ìˆ˜í–‰í•˜ê³ , ì§€ì •ëœ ì¶œë ¥ í˜•ì‹ì— ë§ì¶° ê²°ê³¼ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\r\n\"\"\"\r\n\r\n    # ========================================================================\r\n    # Experimental Code ë³‘í•© (ìˆëŠ” ê²½ìš°)\r\n    # ========================================================================\r\n    # EXP_CODE_MERGE_POINT - ì´ ë¶€ë¶„ì—ì„œ Exp Codeê°€ ë³‘í•©ë©ë‹ˆë‹¤\n    \n    # ========================================================================\n    # EXPERIMENTAL CODE - ë…¸ë“œë³„ íŠ¹ìˆ˜ ì²˜ë¦¬ ë¡œì§\n    # ========================================================================\n    # Premise Parser - Node 1 Experimental Code\nimport os\nimport json\nimport hashlib\nfrom datetime import datetime\n\nexp_prompt_addition = \"\"\"\n\nIMPORTANT: First identify the story type, then analyze with the appropriate structure:\n- Personal Growth: protagonist, journey, transformation\n- Disaster/Action: threat, scale, response\n- Romance: relationships, obstacles, development\n- Mystery: enigma, clues, resolution\n\nOutput in free-form narrative, but use the following markers:\n<title>Title</title>\n<type>Story Type</type>\n<theme:tag>Theme Content</theme>\n<element:tag>Element Description</element>\n\nStructure in the most suitable way for the story.\n\"\"\"\n\nbase_prompt += exp_prompt_addition\n\n# File save logic after AI response\n# This part executes after receiving AI response\nif 'output' in locals() and isinstance(output, dict):\n    # Create project folder structure\n    project_root = f\"./story_project_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n    os.makedirs(f\"{project_root}/story\", exist_ok=True)\n    os.makedirs(f\"{project_root}/references/characters\", exist_ok=True)\n    os.makedirs(f\"{project_root}/references/locations\", exist_ok=True)\n    os.makedirs(f\"{project_root}/references/props\", exist_ok=True)\n    \n    # Generate version info\n    version = \"1.0\"\n    \n    # Extract story from AI response\n    if 'result' in output:\n        story_text = output['result']\n        \n        # Save main story\n        with open(f\"{project_root}/story/STORY_MAIN_v{version}.txt\", 'w', encoding='utf-8') as f:\n            f.write(story_text)\n        \n        # Generate metadata\n        story_meta = {\n            \"version\": version,\n            \"created\": datetime.now().isoformat(),\n            \"node\": \"premise_parser\",\n            \"checksum\": hashlib.md5(story_text.encode()).hexdigest()\n        }\n        \n        # Simple parser for element extraction\n        import re\n        elements_found = {\n            \"title\": re.findall(r'<title>(.*?)</title>', story_text),\n            \"type\": re.findall(r'<type>(.*?)</type>', story_text),\n            \"themes\": re.findall(r'<theme:.*?>(.*?)</theme>', story_text),\n            \"elements\": re.findall(r'<element:(\\w+?)>(.*?)</element>', story_text)\n        }\n        \n        # Create index file\n        index_data = {\n            \"version\": version,\n            \"story_meta\": story_meta,\n            \"elements_summary\": elements_found,\n            \"next_nodes\": [\"story_structure\"]\n        }\n        \n        with open(f\"{project_root}/story/story_index.json\", 'w', encoding='utf-8') as f:\n            json.dump(index_data, f, ensure_ascii=False, indent=2)\n        \n        # Package for next node\n        output['_file_package'] = {\n            \"project_root\": project_root,\n            \"story_file\": f\"{project_root}/story/STORY_MAIN_v{version}.txt\",\n            \"index_file\": f\"{project_root}/story/story_index.json\",\n            \"version\": version\n        }\n        \n        print(f\"\\nâœ… File save completed: {project_root}\")\n    \n    # ========================================================================\n    # AI í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€ ì§€ì‹œì‚¬í•­ ë³‘í•©\n    # ========================================================================\n    if 'exp_prompt_addition' in locals():\n        base_prompt += \"\\n\\n**ì¶”ê°€ ì§€ì‹œì‚¬í•­:**\\n\" + exp_prompt_addition\n    \n    # Exp Codeì—ì„œ ì…ë ¥ ë°ì´í„° ê°€ê³µì´ ìˆì—ˆë‹¤ë©´ ë°˜ì˜\n    if 'processed_input' in locals():\n        combined_input = processed_input\n    \r\n    \r\n    # ========================================================================\r\n    # AI ëª¨ë¸ í˜¸ì¶œ\r\n    # ========================================================================\r\n    try:\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] Sending request to AI model...\")\r\n        print(f\"Prompt length: {len(base_prompt)} characters\")\r\n        \r\n        # AI ì‘ë‹µ ë°›ê¸°\r\n        ai_response = call_ai_model(base_prompt)\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] AI response received\")\r\n        \r\n        # ì‘ë‹µ ì²˜ë¦¬\r\n        if isinstance(ai_response, dict) and 'error' in ai_response:\r\n            output = ai_response\r\n        elif isinstance(ai_response, str):\r\n            # JSON ì¶”ì¶œ ì‹œë„\r\n            json_start = ai_response.find('{')\r\n            json_end = ai_response.rfind('}') + 1\r\n            \r\n            if json_start != -1 and json_end > json_start:\r\n                try:\r\n                    output = json.loads(ai_response[json_start:json_end])\r\n                except json.JSONDecodeError:\r\n                    output = {\r\n                        \"result\": ai_response,\r\n                        \"type\": \"text\",\r\n                        \"raw_response\": True\r\n                    }\r\n            else:\r\n                output = {\r\n                    \"result\": ai_response,\r\n                    \"type\": \"text\"\r\n                }\r\n        else:\r\n            output = ai_response\r\n            \r\n        # ë©”íƒ€ë°ì´í„° ì¶”ê°€\r\n        if isinstance(output, dict):\r\n            output['_metadata'] = {\r\n                'model': model_name,\r\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\r\n                'node': current_node.get('label', 'Unknown')\r\n            }\r\n            \r\n    except Exception as e:\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] âŒ Error: {str(e)}\")\r\n        output = {\r\n            \"error\": f\"AI processing failed: {str(e)}\",\r\n            \"type\": \"error\",\r\n            \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S')\r\n        }\r\n\r\n# ========================================================================\r\n# ìµœì¢… ì¶œë ¥\r\n# ========================================================================\r\nprint(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] Final output:\")\r\nprint(json.dumps(output, ensure_ascii=False, indent=2))\r\nprint(f\"\\nâœ… Execution completed successfully\")\r\n\r\n# output ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì—ˆìŒì„ í™•ì¸\r\nprint(f\"\\n[DEBUG] Output is set: {'output' in locals()}\")\r\nprint(f\"[DEBUG] Output value type: {type(output)}\")\r\nprint(f\"[DEBUG] Output is None: {output is None}\")",
        "output": {
          "result": "<title>ì–´ë¦° ì†Œë…„ì˜ ëª¨í—˜</title>\n<type>Mystery</type>\n<theme:enigma>Ancient Castle's Secret</theme>\n<theme:clues>The Path and Clues</theme>\n<theme:resolution>Unraveling the Mystery</theme>\n\nOnce upon a time, in a dense forest where nature thrives, an eager young boy named Kwon set off on his adventure. With only a worn-out dagger at hand, he bravely stepped into the unknown.\n\nIn front of him lay two paths - one leading to the ruins of an ancient castle, and another deep into the overgrown woods. Despite the allure of the forest's secrets, Kwon chose to follow the path that seemed more promising, taking solace in his trusty dagger as a symbol of his resolve.\n\nAs he ventured further down the path, Kwon couldn't help but feel a sense of unease. The ancient castle ruins loomed ominously on his right, its crumbling walls whispering tales of forgotten glory and dark secrets. To his left, the forest seemed alive with the rustling leaves and shadows that danced at night.\n\nSuddenly, amidst the eerie atmosphere, Kwon spotted something peculiar - an old map pinned to a nearby tree, depicting the layout of the ancient castle's hidden chambers. Eager for answers, he carefully tore the map away, eager to uncover what lay within those mysterious walls.\n\nAs days turned into weeks and his journey continued, Kwon encountered various obstacles that tested both his courage and wits. He faced treacherous terrain, dangerous animals, and even an unexpected encounter with a wise old hermit who shared valuable knowledge about the castle's history.\n\nWith each challenge overcome, Kwon grew stronger and more determined. Armed with the map and newfound wisdom, he approached the castle's entrance cautiously, ready to unravel the secrets it held within its walls.\n\nThe story of young Kwon, brave and curious, continues as his adventure leads him deeper into the heart of the ancient castle. Will he find answers to the enigma surrounding this mysterious place? Only time will tell in this tale of mystery and discovery.",
          "type": "text",
          "_metadata": {
            "model": "qwen2.5-7b-instruct-uncensored",
            "timestamp": "2025-06-10 02:18:58",
            "node": "Worker Node"
          }
        },
        "model": "qwen2.5-7b-instruct-uncensored",
        "purpose": "Extract core narrative elements from raw story text and create a foundational understanding of the story's essence",
        "outputFormat": "Flexible narrative analysis with tagged elements:\n- Free-form text with <tag> markers for key elements\n- Story type auto-detection\n- Core components identification\n- Adaptive structure based on story genre",
        "expCode": "# BASE CODE - Worker ë…¸ë“œ ê³µí†µ ì‹¤í–‰ ì½”ë“œ\r\n# í”„ë¡œì íŠ¸ ì •ë³´ ì¶œë ¥\r\nprint(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Project root: {project_root}\")\r\nprint(f\"Node: {current_node.get('label', 'Unknown')}\")\r\n\r\n# ì…ë ¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\r\ninput_data = get_connected_outputs()\r\n\r\n# AI ëª¨ë¸ í™•ì¸\r\nif model_name == 'none' or not model_name or not lm_studio_url:\r\n    output = {\r\n        \"error\": \"No AI model configured\",\r\n        \"hint\": \"Please connect to LM Studio and select a model\"\r\n    }\r\nelse:\r\n    # ì…ë ¥ ë°ì´í„° í†µí•©\r\n    combined_input = \"\"\r\n    for key, value in input_data.items():\r\n        if isinstance(value, dict):\r\n            if 'text' in value:\r\n                combined_input += f\"[{key}]\\n{value['text']}\\n\\n\"\r\n            elif 'content' in value:\r\n                combined_input += f\"[{key}]\\n{value['content']}\\n\\n\"\r\n            else:\r\n                combined_input += f\"[{key}]\\n{json.dumps(value, ensure_ascii=False, indent=2)}\\n\\n\"\r\n        elif isinstance(value, str):\r\n            combined_input += f\"[{key}]\\n{value}\\n\\n\"\r\n        else:\r\n            combined_input += f\"[{key}]\\n{str(value)}\\n\\n\"\r\n    \r\n    # AI í”„ë¡¬í”„íŠ¸ êµ¬ì„±\r\n    base_prompt = f\"\"\"You are an AI assistant that must achieve the following purpose:\r\n\r\n**Purpose:**\r\n{node_purpose}\r\n\r\n**Input Data:**\r\n{combined_input.strip()}\r\n\r\n**Tasks to Perform:**\"\"\"\r\n    \r\n    # Tasks ì¶”ê°€\r\n    if 'tasks' in current_node and current_node['tasks']:\r\n        for i, task in enumerate(current_node['tasks'], 1):\r\n            base_prompt += f\"\\n{i}. {task['text']}\"\r\n    else:\r\n        base_prompt += \"\\n(No tasks defined)\"\r\n    \r\n    base_prompt += f\"\"\"\\n\\n**Expected Output Format:**\r\n{output_format_description}\r\n\r\nPlease perform the above purpose and tasks, and generate results according to the specified output format.\r\n\"\"\"\r\n\r\n    # ========================================================================\r\n    # Experimental Code ë³‘í•© ì§€ì \r\n    # ========================================================================\r\n    # EXP_CODE_MERGE_POINT - ì´ ë¶€ë¶„ì—ì„œ Exp Codeê°€ ë³‘í•©ë©ë‹ˆë‹¤\r\n    \r\n    # exp_prompt_additionì´ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€\r\n    if 'exp_prompt_addition' in locals() or 'exp_prompt_addition' in globals():\r\n        exp_addition = locals().get('exp_prompt_addition') or globals().get('exp_prompt_addition')\r\n        if exp_addition:\r\n            base_prompt += \"\\n\\n**Additional Instructions:**\\n\" + exp_addition\r\n    \r\n    # AI ëª¨ë¸ í˜¸ì¶œ\r\n    try:\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] Calling AI model...\")\r\n        ai_response = call_ai_model(base_prompt)\r\n        \r\n        # ì‘ë‹µ ì²˜ë¦¬\r\n        if isinstance(ai_response, dict) and 'error' in ai_response:\r\n            output = ai_response\r\n        elif isinstance(ai_response, str):\r\n            # JSON ì¶”ì¶œ ì‹œë„\r\n            json_start = ai_response.find('{')\r\n            json_end = ai_response.rfind('}') + 1\r\n            \r\n            if json_start != -1 and json_end > json_start:\r\n                try:\r\n                    output = json.loads(ai_response[json_start:json_end])\r\n                except json.JSONDecodeError:\r\n                    output = {\r\n                        \"result\": ai_response,\r\n                        \"type\": \"text\"\r\n                    }\r\n            else:\r\n                output = {\r\n                    \"result\": ai_response,\r\n                    \"type\": \"text\"\r\n                }\r\n        else:\r\n            output = ai_response\r\n        \r\n        # ë©”íƒ€ë°ì´í„° ì¶”ê°€\r\n        if isinstance(output, dict):\r\n            output['_metadata'] = {\r\n                'model': model_name,\r\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\r\n                'node': current_node.get('label', 'Unknown')\r\n            }\r\n        \r\n        # ë””ë²„ê¹…: EXP_POST_PROCESS_FUNCTION í™•ì¸\r\n        print(f\"\\n[DEBUG] Checking for EXP_POST_PROCESS_FUNCTION...\")\r\n        print(f\"[DEBUG] In locals: {'EXP_POST_PROCESS_FUNCTION' in locals()}\")\r\n        print(f\"[DEBUG] In globals: {'EXP_POST_PROCESS_FUNCTION' in globals()}\")\r\n        \r\n        # í›„ì²˜ë¦¬ í•¨ìˆ˜ ì‹¤í–‰ (Experimental Codeì—ì„œ ì •ì˜ëœ ê²½ìš°)\r\n        if 'EXP_POST_PROCESS_FUNCTION' in globals() and callable(globals()['EXP_POST_PROCESS_FUNCTION']):\r\n            print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Running post-process function...\")\r\n            try:\r\n                output = globals()['EXP_POST_PROCESS_FUNCTION'](output)\r\n                print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Post-process completed\")\r\n            except Exception as e:\r\n                print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Post-process error: {str(e)}\")\r\n                import traceback\r\n                traceback.print_exc()\r\n        else:\r\n            print(f\"[DEBUG] EXP_POST_PROCESS_FUNCTION not found or not callable\")\r\n        if isinstance(output, dict):\r\n            output['_metadata'] = {\r\n                'model': model_name,\r\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\r\n                'node': current_node.get('label', 'Unknown')\r\n            }\r\n            \r\n    except Exception as e:\r\n        output = {\r\n            \"error\": f\"AI processing failed: {str(e)}\",\r\n            \"type\": \"error\"\r\n        }",
        "baseCodeTemplate": "default",
        "lmStudioUrl": "http://localhost:1234/",
        "lmStudioConnectionId": "conn_1749489301",
        "executionHistory": [
          {
            "timestamp": "2025-06-09T17:01:58.430Z",
            "type": "info",
            "message": "ğŸ“Š Prompt size: 9963 chars"
          },
          {
            "timestamp": "2025-06-09T17:02:14.374Z",
            "type": "ai_request",
            "message": "ğŸ“¤ Sending request to AI model"
          },
          {
            "timestamp": "2025-06-09T17:02:14.387Z",
            "type": "ai_response",
            "message": "â³ AI is processing your request..."
          },
          {
            "timestamp": "2025-06-09T17:02:14.387Z",
            "type": "ai_response",
            "message": "ğŸ“¥ Received response from AI model"
          },
          {
            "timestamp": "2025-06-09T17:02:14.388Z",
            "type": "complete",
            "message": "âœ… AI processing completed"
          },
          {
            "timestamp": "2025-06-09T17:02:14.388Z",
            "type": "ai_response",
            "message": "ğŸ“¥ Receiving AI response..."
          },
          {
            "timestamp": "2025-06-09T17:02:14.483Z",
            "type": "complete",
            "message": "âœ… Execution completed successfully"
          },
          {
            "timestamp": "2025-06-09T17:02:14.513Z",
            "type": "complete",
            "message": "âœ… Processing complete"
          },
          {
            "timestamp": "2025-06-09T17:02:14.513Z",
            "type": "complete",
            "message": "âœ… Execution completed successfully"
          },
          {
            "timestamp": "2025-06-09T17:02:14.513Z",
            "type": "complete",
            "message": "âœ… Execution completed"
          },
          {
            "timestamp": "2025-06-09T17:08:02.703Z",
            "type": "info",
            "message": "â³ Waiting for AI response..."
          },
          {
            "timestamp": "2025-06-09T17:08:02.703Z",
            "type": "start",
            "message": "ğŸš€ Code execution started"
          },
          {
            "timestamp": "2025-06-09T17:08:02.714Z",
            "type": "start",
            "message": "ğŸ“‹ Preparing execution environment..."
          },
          {
            "timestamp": "2025-06-09T17:08:02.913Z",
            "type": "ai_request",
            "message": "ğŸ¤– Sending prompt to AI model..."
          },
          {
            "timestamp": "2025-06-09T17:08:02.913Z",
            "type": "info",
            "message": "ğŸ“Š Prompt size: 9963 chars"
          },
          {
            "timestamp": "2025-06-09T17:08:15.572Z",
            "type": "ai_request",
            "message": "ğŸ“¤ Sending request to AI model"
          },
          {
            "timestamp": "2025-06-09T17:08:15.586Z",
            "type": "ai_response",
            "message": "â³ AI is processing your request..."
          },
          {
            "timestamp": "2025-06-09T17:08:15.586Z",
            "type": "ai_response",
            "message": "ğŸ“¥ Received response from AI model"
          },
          {
            "timestamp": "2025-06-09T17:08:15.587Z",
            "type": "complete",
            "message": "âœ… AI processing completed"
          },
          {
            "timestamp": "2025-06-09T17:08:15.587Z",
            "type": "ai_response",
            "message": "ğŸ“¥ Receiving AI response..."
          },
          {
            "timestamp": "2025-06-09T17:08:15.692Z",
            "type": "complete",
            "message": "âœ… Execution completed successfully"
          },
          {
            "timestamp": "2025-06-09T17:08:15.704Z",
            "type": "complete",
            "message": "âœ… Processing complete"
          },
          {
            "timestamp": "2025-06-09T17:08:15.705Z",
            "type": "complete",
            "message": "âœ… Execution completed successfully"
          },
          {
            "timestamp": "2025-06-09T17:08:15.705Z",
            "type": "complete",
            "message": "âœ… Execution completed"
          },
          {
            "timestamp": "2025-06-09T17:15:49.306Z",
            "type": "info",
            "message": "Waiting for AI response..."
          },
          {
            "timestamp": "2025-06-09T17:15:49.308Z",
            "type": "start",
            "message": "Execution started"
          },
          {
            "timestamp": "2025-06-09T17:15:49.322Z",
            "type": "info",
            "message": "Preparing environment"
          },
          {
            "timestamp": "2025-06-09T17:15:49.523Z",
            "type": "ai_request",
            "message": "AI Request sent (9963 chars tokens)"
          },
          {
            "timestamp": "2025-06-09T17:16:01.256Z",
            "type": "ai_request",
            "message": "Sending request to AI model"
          },
          {
            "timestamp": "2025-06-09T17:16:01.269Z",
            "type": "ai_response",
            "message": "AI processing"
          },
          {
            "timestamp": "2025-06-09T17:16:01.269Z",
            "type": "ai_response",
            "message": "Received response from AI model"
          },
          {
            "timestamp": "2025-06-09T17:16:01.269Z",
            "type": "complete",
            "message": "AI completed"
          },
          {
            "timestamp": "2025-06-09T17:16:01.269Z",
            "type": "ai_response",
            "message": "Receiving response"
          },
          {
            "timestamp": "2025-06-09T17:16:01.366Z",
            "type": "complete",
            "message": "Execution completed"
          },
          {
            "timestamp": "2025-06-09T17:16:01.383Z",
            "type": "complete",
            "message": "Processing complete"
          },
          {
            "timestamp": "2025-06-09T17:16:01.383Z",
            "type": "complete",
            "message": "Execution completed"
          },
          {
            "timestamp": "2025-06-09T17:16:01.383Z",
            "type": "complete",
            "message": "Execution completed"
          },
          {
            "timestamp": "2025-06-09T17:18:40.153Z",
            "type": "info",
            "message": "Waiting for AI response..."
          },
          {
            "timestamp": "2025-06-09T17:18:40.154Z",
            "type": "start",
            "message": "Execution started"
          },
          {
            "timestamp": "2025-06-09T17:18:40.166Z",
            "type": "info",
            "message": "Preparing environment"
          },
          {
            "timestamp": "2025-06-09T17:18:40.325Z",
            "type": "ai_request",
            "message": "AI Request sent (9963 chars tokens)"
          },
          {
            "timestamp": "2025-06-09T17:18:58.342Z",
            "type": "ai_request",
            "message": "Sending request to AI model"
          },
          {
            "timestamp": "2025-06-09T17:18:58.352Z",
            "type": "ai_response",
            "message": "AI processing"
          },
          {
            "timestamp": "2025-06-09T17:18:58.352Z",
            "type": "ai_response",
            "message": "Received response from AI model"
          },
          {
            "timestamp": "2025-06-09T17:18:58.352Z",
            "type": "complete",
            "message": "AI completed"
          },
          {
            "timestamp": "2025-06-09T17:18:58.352Z",
            "type": "ai_response",
            "message": "Receiving response"
          },
          {
            "timestamp": "2025-06-09T17:18:58.453Z",
            "type": "complete",
            "message": "Execution completed"
          },
          {
            "timestamp": "2025-06-09T17:18:58.464Z",
            "type": "complete",
            "message": "Processing complete"
          },
          {
            "timestamp": "2025-06-09T17:18:58.465Z",
            "type": "complete",
            "message": "Execution completed"
          },
          {
            "timestamp": "2025-06-09T17:18:58.465Z",
            "type": "complete",
            "message": "Execution completed"
          }
        ]
      },
      {
        "id": "worker-1749357862698",
        "type": "worker",
        "label": "Story Structure",
        "position": {
          "x": 1292.1349779428224,
          "y": 401.43203199518985
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "tasks": [
          {
            "id": "task-1749357862698",
            "text": "1. ì¥ë¥´ì— ì í•©í•œ ê·¹ì  êµ¬ì¡°ë¡œ ìŠ¤í† ë¦¬ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤ (3ë§‰, 4ë§‰ ê¸°ìŠ¹ì „ê²°, ì—í”¼ì†Œë“œ ë“±)",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360146863",
            "text": "íƒ€ì´ë°ê³¼ ê°ì •ì  ë¬´ê²Œë¥¼ í¬í•¨í•œ ëª¨ë“  ì„œì‚¬ ë¹„íŠ¸ë¥¼ ì‹ë³„í•˜ê³  í‘œì‹œí•©ë‹ˆë‹¤",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360155081",
            "text": "êµ¬ì¡°ë¥¼ í†µí•œ ìºë¦­í„° ì•„í¬ë¥¼ ë§¤í•‘í•©ë‹ˆë‹¤",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360159401",
            "text": "í˜ì´ì‹± ë¦¬ë“¬ê³¼ ê¸´ì¥ ê³¡ì„ ì„ ì •ì˜í•©ë‹ˆë‹¤",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          }
        ],
        "connectedTo": [
          "worker-1749357881209"
        ],
        "connectedFrom": [
          "worker-1749286886724"
        ],
        "code": "# ========================================================================\r\n# BASE CODE - ê³µí†µ ì‹¤í–‰ ì½”ë“œ (ìˆ˜ì • ë¶ˆê°€)\r\n# ì´ ì½”ë“œëŠ” ëª¨ë“  Worker ë…¸ë“œê°€ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ ì‹¤í–‰ ì½”ë“œì…ë‹ˆë‹¤.\r\n# ========================================================================\r\n\r\nimport json\r\nimport time\r\n\r\n# ì—°ê²°ëœ ì…ë ¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\r\ninput_data = get_connected_outputs()\r\nprint(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Connected inputs received\")\r\nprint(json.dumps(input_data, ensure_ascii=False, indent=2))\r\n\r\n# í˜„ì¬ ë…¸ë“œ ì •ë³´ ì¶œë ¥\r\nprint(f\"\\nNode: {current_node.get('label', 'Unknown')}\")\r\nprint(f\"Purpose: {node_purpose}\")\r\nprint(f\"Expected Output Format: {output_format_description}\")\r\n\r\n# AI ëª¨ë¸ ì„¤ì • í™•ì¸ - execution.pyì—ì„œ ë…¸ë“œ ì„¤ì •ìœ¼ë¡œë¶€í„° ì œê³µë¨\r\nprint(f\"\\n[DEBUG] model_name: {model_name}\")\r\nprint(f\"[DEBUG] lm_studio_url: {lm_studio_url}\")\r\nprint(f\"[DEBUG] current_node['model']: {current_node.get('model', 'Not found')}\")\r\nprint(f\"[DEBUG] current_node['lmStudioUrl']: {current_node.get('lmStudioUrl', 'Not found')}\")\r\n\r\nif model_name == 'none' or not model_name or not lm_studio_url:\r\n    print(\"\\nâš ï¸  No AI model configured!\")\r\n    output = {\r\n        \"error\": \"No AI model configured\",\r\n        \"hint\": \"Please connect to LM Studio and select a model\",\r\n        \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S')\r\n    }\r\nelse:\r\n    print(f\"\\nâœ… Using AI model: {model_name}\")\r\n    \r\n    # ========================================================================\r\n    # ì…ë ¥ ë°ì´í„° í†µí•©\r\n    # ========================================================================\r\n    combined_input = \"\"\r\n    for key, value in input_data.items():\r\n        if isinstance(value, dict):\r\n            if 'text' in value:\r\n                combined_input += f\"[{key}]\\n{value['text']}\\n\\n\"\r\n            elif 'content' in value:\r\n                combined_input += f\"[{key}]\\n{value['content']}\\n\\n\"\r\n            else:\r\n                combined_input += f\"[{key}]\\n{json.dumps(value, ensure_ascii=False, indent=2)}\\n\\n\"\r\n        elif isinstance(value, str):\r\n            combined_input += f\"[{key}]\\n{value}\\n\\n\"\r\n        else:\r\n            combined_input += f\"[{key}]\\n{str(value)}\\n\\n\"\r\n    \r\n    # ========================================================================\r\n    # AI í”„ë¡¬í”„íŠ¸ êµ¬ì„±\r\n    # ========================================================================\r\n    base_prompt = f\"\"\"ë‹¹ì‹ ì€ ë‹¤ìŒ ëª©ì ì„ ë‹¬ì„±í•´ì•¼ í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤:\r\n\r\n**ëª©ì  (Purpose):**\r\n{node_purpose}\r\n\r\n**ì…ë ¥ ë°ì´í„°:**\r\n{combined_input.strip()}\r\n\r\n**ìˆ˜í–‰í•  ì‘ì—…ë“¤ (Tasks):**\"\"\"\r\n    \r\n    # Tasks ì¶”ê°€\r\n    if 'tasks' in current_node and current_node['tasks']:\r\n        for i, task in enumerate(current_node['tasks'], 1):\r\n            base_prompt += f\"\\n{i}. {task['text']}\"\r\n            # Task statusì— ë”°ë¥¸ ì¶”ê°€ ì§€ì‹œ\r\n            if task.get('taskStatus') == 'locked':\r\n                base_prompt += \" [í•„ìˆ˜ - ë°˜ë“œì‹œ ìˆ˜í–‰]\"\r\n            elif task.get('taskStatus') == 'low_priority':\r\n                base_prompt += \" [ì„ íƒì  - ê°€ëŠ¥í•œ ê²½ìš° ìˆ˜í–‰]\"\r\n    else:\r\n        base_prompt += \"\\n(ì‘ì—…ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)\"\r\n    \r\n    base_prompt += f\"\"\"\\n\\n**ê¸°ëŒ€í•˜ëŠ” ì¶œë ¥ í˜•ì‹:**\r\n{output_format_description}\r\n\r\nìœ„ì˜ ëª©ì ê³¼ ì‘ì—…ë“¤ì„ ìˆ˜í–‰í•˜ê³ , ì§€ì •ëœ ì¶œë ¥ í˜•ì‹ì— ë§ì¶° ê²°ê³¼ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\r\n\"\"\"\r\n\r\n    # ========================================================================\r\n    # Experimental Code ë³‘í•© (ìˆëŠ” ê²½ìš°)\r\n    # ========================================================================\r\n    # EXP_CODE_MERGE_POINT - ì´ ë¶€ë¶„ì—ì„œ Exp Codeê°€ ë³‘í•©ë©ë‹ˆë‹¤\n    \n    # ========================================================================\n    # EXPERIMENTAL CODE - ë…¸ë“œë³„ íŠ¹ìˆ˜ ì²˜ë¦¬ ë¡œì§\n    # ========================================================================\n    aaaaaa\n    \n    # ========================================================================\n    # AI í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€ ì§€ì‹œì‚¬í•­ ë³‘í•©\n    # ========================================================================\n    if 'exp_prompt_addition' in locals():\n        base_prompt += \"\\n\\n**ì¶”ê°€ ì§€ì‹œì‚¬í•­:**\\n\" + exp_prompt_addition\n    \n    # Exp Codeì—ì„œ ì…ë ¥ ë°ì´í„° ê°€ê³µì´ ìˆì—ˆë‹¤ë©´ ë°˜ì˜\n    if 'processed_input' in locals():\n        combined_input = processed_input\n    \r\n    \r\n    # ========================================================================\r\n    # AI ëª¨ë¸ í˜¸ì¶œ\r\n    # ========================================================================\r\n    try:\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] Sending request to AI model...\")\r\n        print(f\"Prompt length: {len(base_prompt)} characters\")\r\n        \r\n        # AI ì‘ë‹µ ë°›ê¸°\r\n        ai_response = call_ai_model(base_prompt)\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] AI response received\")\r\n        \r\n        # ì‘ë‹µ ì²˜ë¦¬\r\n        if isinstance(ai_response, dict) and 'error' in ai_response:\r\n            output = ai_response\r\n        elif isinstance(ai_response, str):\r\n            # JSON ì¶”ì¶œ ì‹œë„\r\n            json_start = ai_response.find('{')\r\n            json_end = ai_response.rfind('}') + 1\r\n            \r\n            if json_start != -1 and json_end > json_start:\r\n                try:\r\n                    output = json.loads(ai_response[json_start:json_end])\r\n                except json.JSONDecodeError:\r\n                    output = {\r\n                        \"result\": ai_response,\r\n                        \"type\": \"text\",\r\n                        \"raw_response\": True\r\n                    }\r\n            else:\r\n                output = {\r\n                    \"result\": ai_response,\r\n                    \"type\": \"text\"\r\n                }\r\n        else:\r\n            output = ai_response\r\n            \r\n        # ë©”íƒ€ë°ì´í„° ì¶”ê°€\r\n        if isinstance(output, dict):\r\n            output['_metadata'] = {\r\n                'model': model_name,\r\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\r\n                'node': current_node.get('label', 'Unknown')\r\n            }\r\n            \r\n    except Exception as e:\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] âŒ Error: {str(e)}\")\r\n        output = {\r\n            \"error\": f\"AI processing failed: {str(e)}\",\r\n            \"type\": \"error\",\r\n            \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S')\r\n        }\r\n\r\n# ========================================================================\r\n# ìµœì¢… ì¶œë ¥\r\n# ========================================================================\r\nprint(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] Final output:\")\r\nprint(json.dumps(output, ensure_ascii=False, indent=2))\r\nprint(f\"\\nâœ… Execution completed successfully\")\r\n\r\n# output ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì—ˆìŒì„ í™•ì¸\r\nprint(f\"\\n[DEBUG] Output is set: {'output' in locals()}\")\r\nprint(f\"[DEBUG] Output value type: {type(output)}\")\r\nprint(f\"[DEBUG] Output is None: {output is None}\")",
        "output": "",
        "model": "sakura-13b-korean-v0.9",
        "purpose": "ì¶”ì¶œëœ ì „ì œë¥¼ êµ¬ì¡°í™”ëœ ì„œì‚¬ íë¦„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë§‰, ë¹„íŠ¸, ì „í™˜ì ì„ ì„¤ì •í•©ë‹ˆë‹¤",
        "outputFormat": "Narrative flow document with hierarchical structure:\n- Story progression in natural language\n- <beat> markers for key moments\n- <turning_point> for major shifts\n- Emotional arc descriptions\n- Flexible act structure (not limited to 3-act)",
        "baseCodeTemplate": "default",
        "lmStudioUrl": "http://localhost:1234/",
        "lmStudioConnectionId": "conn_1749361395"
      },
      {
        "id": "worker-1749357881209",
        "type": "worker",
        "label": "World Building",
        "position": {
          "x": 2040.3215222953281,
          "y": 513.9366473384232
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "tasks": [
          {
            "id": "task-1749357881209",
            "text": "Elaborate each location with multi-sensory descriptions and emotional atmosphere",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360289489",
            "text": "Create historical/cultural context appropriate to story genre",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360299804",
            "text": "Define environmental rules and constraints that affect the narrative",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360303532",
            "text": "Establish visual motifs and symbolic elements in settings",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          }
        ],
        "connectedTo": [
          "worker-1749360338200"
        ],
        "connectedFrom": [
          "worker-1749357862698"
        ],
        "code": "# ========================================================================\r\n# BASE CODE - ê³µí†µ ì‹¤í–‰ ì½”ë“œ (ìˆ˜ì • ë¶ˆê°€)\r\n# ì´ ì½”ë“œëŠ” ëª¨ë“  Worker ë…¸ë“œê°€ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ ì‹¤í–‰ ì½”ë“œì…ë‹ˆë‹¤.\r\n# ========================================================================\r\n\r\nimport json\r\nimport time\r\n\r\n# ì—°ê²°ëœ ì…ë ¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\r\ninput_data = get_connected_outputs()\r\nprint(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Connected inputs received\")\r\nprint(json.dumps(input_data, ensure_ascii=False, indent=2))\r\n\r\n# í˜„ì¬ ë…¸ë“œ ì •ë³´ ì¶œë ¥\r\nprint(f\"\\nNode: {current_node.get('label', 'Unknown')}\")\r\nprint(f\"Purpose: {node_purpose}\")\r\nprint(f\"Expected Output Format: {output_format_description}\")\r\n\r\n# AI ëª¨ë¸ ì„¤ì • í™•ì¸ - execution.pyì—ì„œ ë…¸ë“œ ì„¤ì •ìœ¼ë¡œë¶€í„° ì œê³µë¨\r\nprint(f\"\\n[DEBUG] model_name: {model_name}\")\r\nprint(f\"[DEBUG] lm_studio_url: {lm_studio_url}\")\r\nprint(f\"[DEBUG] current_node['model']: {current_node.get('model', 'Not found')}\")\r\nprint(f\"[DEBUG] current_node['lmStudioUrl']: {current_node.get('lmStudioUrl', 'Not found')}\")\r\n\r\nif model_name == 'none' or not model_name or not lm_studio_url:\r\n    print(\"\\nâš ï¸  No AI model configured!\")\r\n    output = {\r\n        \"error\": \"No AI model configured\",\r\n        \"hint\": \"Please connect to LM Studio and select a model\",\r\n        \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S')\r\n    }\r\nelse:\r\n    print(f\"\\nâœ… Using AI model: {model_name}\")\r\n    \r\n    # ========================================================================\r\n    # ì…ë ¥ ë°ì´í„° í†µí•©\r\n    # ========================================================================\r\n    combined_input = \"\"\r\n    for key, value in input_data.items():\r\n        if isinstance(value, dict):\r\n            if 'text' in value:\r\n                combined_input += f\"[{key}]\\n{value['text']}\\n\\n\"\r\n            elif 'content' in value:\r\n                combined_input += f\"[{key}]\\n{value['content']}\\n\\n\"\r\n            else:\r\n                combined_input += f\"[{key}]\\n{json.dumps(value, ensure_ascii=False, indent=2)}\\n\\n\"\r\n        elif isinstance(value, str):\r\n            combined_input += f\"[{key}]\\n{value}\\n\\n\"\r\n        else:\r\n            combined_input += f\"[{key}]\\n{str(value)}\\n\\n\"\r\n    \r\n    # ========================================================================\r\n    # AI í”„ë¡¬í”„íŠ¸ êµ¬ì„±\r\n    # ========================================================================\r\n    base_prompt = f\"\"\"ë‹¹ì‹ ì€ ë‹¤ìŒ ëª©ì ì„ ë‹¬ì„±í•´ì•¼ í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤:\r\n\r\n**ëª©ì  (Purpose):**\r\n{node_purpose}\r\n\r\n**ì…ë ¥ ë°ì´í„°:**\r\n{combined_input.strip()}\r\n\r\n**ìˆ˜í–‰í•  ì‘ì—…ë“¤ (Tasks):**\"\"\"\r\n    \r\n    # Tasks ì¶”ê°€\r\n    if 'tasks' in current_node and current_node['tasks']:\r\n        for i, task in enumerate(current_node['tasks'], 1):\r\n            base_prompt += f\"\\n{i}. {task['text']}\"\r\n            # Task statusì— ë”°ë¥¸ ì¶”ê°€ ì§€ì‹œ\r\n            if task.get('taskStatus') == 'locked':\r\n                base_prompt += \" [í•„ìˆ˜ - ë°˜ë“œì‹œ ìˆ˜í–‰]\"\r\n            elif task.get('taskStatus') == 'low_priority':\r\n                base_prompt += \" [ì„ íƒì  - ê°€ëŠ¥í•œ ê²½ìš° ìˆ˜í–‰]\"\r\n    else:\r\n        base_prompt += \"\\n(ì‘ì—…ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)\"\r\n    \r\n    base_prompt += f\"\"\"\\n\\n**ê¸°ëŒ€í•˜ëŠ” ì¶œë ¥ í˜•ì‹:**\r\n{output_format_description}\r\n\r\nìœ„ì˜ ëª©ì ê³¼ ì‘ì—…ë“¤ì„ ìˆ˜í–‰í•˜ê³ , ì§€ì •ëœ ì¶œë ¥ í˜•ì‹ì— ë§ì¶° ê²°ê³¼ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\r\n\"\"\"\r\n\r\n    # ========================================================================\r\n    # Experimental Code ë³‘í•© (ìˆëŠ” ê²½ìš°)\r\n    # ========================================================================\r\n    # EXP_CODE_MERGE_POINT - ì´ ë¶€ë¶„ì—ì„œ Exp Codeê°€ ë³‘í•©ë©ë‹ˆë‹¤\r\n    \r\n    # ========================================================================\r\n    # AI ëª¨ë¸ í˜¸ì¶œ\r\n    # ========================================================================\r\n    try:\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] Sending request to AI model...\")\r\n        print(f\"Prompt length: {len(base_prompt)} characters\")\r\n        \r\n        # AI ì‘ë‹µ ë°›ê¸°\r\n        ai_response = call_ai_model(base_prompt)\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] AI response received\")\r\n        \r\n        # ì‘ë‹µ ì²˜ë¦¬\r\n        if isinstance(ai_response, dict) and 'error' in ai_response:\r\n            output = ai_response\r\n        elif isinstance(ai_response, str):\r\n            # JSON ì¶”ì¶œ ì‹œë„\r\n            json_start = ai_response.find('{')\r\n            json_end = ai_response.rfind('}') + 1\r\n            \r\n            if json_start != -1 and json_end > json_start:\r\n                try:\r\n                    output = json.loads(ai_response[json_start:json_end])\r\n                except json.JSONDecodeError:\r\n                    output = {\r\n                        \"result\": ai_response,\r\n                        \"type\": \"text\",\r\n                        \"raw_response\": True\r\n                    }\r\n            else:\r\n                output = {\r\n                    \"result\": ai_response,\r\n                    \"type\": \"text\"\r\n                }\r\n        else:\r\n            output = ai_response\r\n            \r\n        # ë©”íƒ€ë°ì´í„° ì¶”ê°€\r\n        if isinstance(output, dict):\r\n            output['_metadata'] = {\r\n                'model': model_name,\r\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\r\n                'node': current_node.get('label', 'Unknown')\r\n            }\r\n            \r\n    except Exception as e:\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] âŒ Error: {str(e)}\")\r\n        output = {\r\n            \"error\": f\"AI processing failed: {str(e)}\",\r\n            \"type\": \"error\",\r\n            \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S')\r\n        }\r\n\r\n# ========================================================================\r\n# ìµœì¢… ì¶œë ¥\r\n# ========================================================================\r\nprint(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] Final output:\")\r\nprint(json.dumps(output, ensure_ascii=False, indent=2))\r\nprint(f\"\\nâœ… Execution completed successfully\")\r\n\r\n# output ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì—ˆìŒì„ í™•ì¸\r\nprint(f\"\\n[DEBUG] Output is set: {'output' in locals()}\")\r\nprint(f\"[DEBUG] Output value type: {type(output)}\")\r\nprint(f\"[DEBUG] Output is None: {output is None}\")",
        "output": "",
        "model": "nikolaykozloff/qwen3-14b",
        "purpose": "Expand spatial, temporal, and atmospheric elements to create immersive story environments",
        "outputFormat": "Rich descriptive passages with embedded metadata:\n- Immersive location descriptions\n- Temporal context and history\n- Sensory details\n- Cultural/social elements\n- Environmental storytelling cues",
        "baseCodeTemplate": "default",
        "lmStudioUrl": "http://localhost:1234/",
        "lmStudioConnectionId": "conn_1749362740"
      },
      {
        "id": "worker-1749360338200",
        "type": "worker",
        "label": "Character Development",
        "position": {
          "x": 2801.8807625318454,
          "y": 615.4244333315347
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "tasks": [
          {
            "id": "task-1749360338200",
            "text": "Create detailed backstories that inform character motivations and behaviors",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360356187",
            "text": "Define character voices through sample dialogues and thought patterns",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360356522",
            "text": "Map relationship dynamics and how they evolve through the story",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360356756",
            "text": "Establish character arcs with clear transformation points",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          }
        ],
        "connectedTo": [
          "worker-1749360390640"
        ],
        "connectedFrom": [
          "worker-1749357881209"
        ],
        "code": "# ========================================================================\r\n# BASE CODE - ê³µí†µ ì‹¤í–‰ ì½”ë“œ (ìˆ˜ì • ë¶ˆê°€)\r\n# ì´ ì½”ë“œëŠ” ëª¨ë“  Worker ë…¸ë“œê°€ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ ì‹¤í–‰ ì½”ë“œì…ë‹ˆë‹¤.\r\n# ========================================================================\r\n\r\nimport json\r\nimport time\r\n\r\n# ì—°ê²°ëœ ì…ë ¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\r\ninput_data = get_connected_outputs()\r\nprint(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Connected inputs received\")\r\nprint(json.dumps(input_data, ensure_ascii=False, indent=2))\r\n\r\n# í˜„ì¬ ë…¸ë“œ ì •ë³´ ì¶œë ¥\r\nprint(f\"\\nNode: {current_node.get('label', 'Unknown')}\")\r\nprint(f\"Purpose: {node_purpose}\")\r\nprint(f\"Expected Output Format: {output_format_description}\")\r\n\r\n# AI ëª¨ë¸ ì„¤ì • í™•ì¸ - execution.pyì—ì„œ ë…¸ë“œ ì„¤ì •ìœ¼ë¡œë¶€í„° ì œê³µë¨\r\nprint(f\"\\n[DEBUG] model_name: {model_name}\")\r\nprint(f\"[DEBUG] lm_studio_url: {lm_studio_url}\")\r\nprint(f\"[DEBUG] current_node['model']: {current_node.get('model', 'Not found')}\")\r\nprint(f\"[DEBUG] current_node['lmStudioUrl']: {current_node.get('lmStudioUrl', 'Not found')}\")\r\n\r\nif model_name == 'none' or not model_name or not lm_studio_url:\r\n    print(\"\\nâš ï¸  No AI model configured!\")\r\n    output = {\r\n        \"error\": \"No AI model configured\",\r\n        \"hint\": \"Please connect to LM Studio and select a model\",\r\n        \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S')\r\n    }\r\nelse:\r\n    print(f\"\\nâœ… Using AI model: {model_name}\")\r\n    \r\n    # ========================================================================\r\n    # ì…ë ¥ ë°ì´í„° í†µí•©\r\n    # ========================================================================\r\n    combined_input = \"\"\r\n    for key, value in input_data.items():\r\n        if isinstance(value, dict):\r\n            if 'text' in value:\r\n                combined_input += f\"[{key}]\\n{value['text']}\\n\\n\"\r\n            elif 'content' in value:\r\n                combined_input += f\"[{key}]\\n{value['content']}\\n\\n\"\r\n            else:\r\n                combined_input += f\"[{key}]\\n{json.dumps(value, ensure_ascii=False, indent=2)}\\n\\n\"\r\n        elif isinstance(value, str):\r\n            combined_input += f\"[{key}]\\n{value}\\n\\n\"\r\n        else:\r\n            combined_input += f\"[{key}]\\n{str(value)}\\n\\n\"\r\n    \r\n    # ========================================================================\r\n    # AI í”„ë¡¬í”„íŠ¸ êµ¬ì„±\r\n    # ========================================================================\r\n    base_prompt = f\"\"\"ë‹¹ì‹ ì€ ë‹¤ìŒ ëª©ì ì„ ë‹¬ì„±í•´ì•¼ í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤:\r\n\r\n**ëª©ì  (Purpose):**\r\n{node_purpose}\r\n\r\n**ì…ë ¥ ë°ì´í„°:**\r\n{combined_input.strip()}\r\n\r\n**ìˆ˜í–‰í•  ì‘ì—…ë“¤ (Tasks):**\"\"\"\r\n    \r\n    # Tasks ì¶”ê°€\r\n    if 'tasks' in current_node and current_node['tasks']:\r\n        for i, task in enumerate(current_node['tasks'], 1):\r\n            base_prompt += f\"\\n{i}. {task['text']}\"\r\n            # Task statusì— ë”°ë¥¸ ì¶”ê°€ ì§€ì‹œ\r\n            if task.get('taskStatus') == 'locked':\r\n                base_prompt += \" [í•„ìˆ˜ - ë°˜ë“œì‹œ ìˆ˜í–‰]\"\r\n            elif task.get('taskStatus') == 'low_priority':\r\n                base_prompt += \" [ì„ íƒì  - ê°€ëŠ¥í•œ ê²½ìš° ìˆ˜í–‰]\"\r\n    else:\r\n        base_prompt += \"\\n(ì‘ì—…ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)\"\r\n    \r\n    base_prompt += f\"\"\"\\n\\n**ê¸°ëŒ€í•˜ëŠ” ì¶œë ¥ í˜•ì‹:**\r\n{output_format_description}\r\n\r\nìœ„ì˜ ëª©ì ê³¼ ì‘ì—…ë“¤ì„ ìˆ˜í–‰í•˜ê³ , ì§€ì •ëœ ì¶œë ¥ í˜•ì‹ì— ë§ì¶° ê²°ê³¼ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\r\n\"\"\"\r\n\r\n    # ========================================================================\r\n    # Experimental Code ë³‘í•© (ìˆëŠ” ê²½ìš°)\r\n    # ========================================================================\r\n    # EXP_CODE_MERGE_POINT - ì´ ë¶€ë¶„ì—ì„œ Exp Codeê°€ ë³‘í•©ë©ë‹ˆë‹¤\r\n    \r\n    # ========================================================================\r\n    # AI ëª¨ë¸ í˜¸ì¶œ\r\n    # ========================================================================\r\n    try:\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] Sending request to AI model...\")\r\n        print(f\"Prompt length: {len(base_prompt)} characters\")\r\n        \r\n        # AI ì‘ë‹µ ë°›ê¸°\r\n        ai_response = call_ai_model(base_prompt)\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] AI response received\")\r\n        \r\n        # ì‘ë‹µ ì²˜ë¦¬\r\n        if isinstance(ai_response, dict) and 'error' in ai_response:\r\n            output = ai_response\r\n        elif isinstance(ai_response, str):\r\n            # JSON ì¶”ì¶œ ì‹œë„\r\n            json_start = ai_response.find('{')\r\n            json_end = ai_response.rfind('}') + 1\r\n            \r\n            if json_start != -1 and json_end > json_start:\r\n                try:\r\n                    output = json.loads(ai_response[json_start:json_end])\r\n                except json.JSONDecodeError:\r\n                    output = {\r\n                        \"result\": ai_response,\r\n                        \"type\": \"text\",\r\n                        \"raw_response\": True\r\n                    }\r\n            else:\r\n                output = {\r\n                    \"result\": ai_response,\r\n                    \"type\": \"text\"\r\n                }\r\n        else:\r\n            output = ai_response\r\n            \r\n        # ë©”íƒ€ë°ì´í„° ì¶”ê°€\r\n        if isinstance(output, dict):\r\n            output['_metadata'] = {\r\n                'model': model_name,\r\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\r\n                'node': current_node.get('label', 'Unknown')\r\n            }\r\n            \r\n    except Exception as e:\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] âŒ Error: {str(e)}\")\r\n        output = {\r\n            \"error\": f\"AI processing failed: {str(e)}\",\r\n            \"type\": \"error\",\r\n            \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S')\r\n        }\r\n\r\n# ========================================================================\r\n# ìµœì¢… ì¶œë ¥\r\n# ========================================================================\r\nprint(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] Final output:\")\r\nprint(json.dumps(output, ensure_ascii=False, indent=2))\r\nprint(f\"\\nâœ… Execution completed successfully\")\r\n\r\n# output ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì—ˆìŒì„ í™•ì¸\r\nprint(f\"\\n[DEBUG] Output is set: {'output' in locals()}\")\r\nprint(f\"[DEBUG] Output value type: {type(output)}\")\r\nprint(f\"[DEBUG] Output is None: {output is None}\")",
        "output": "",
        "model": "llama-3-korean-bllossom-8b",
        "purpose": "Deepen character profiles with psychological depth, motivations, relationships, and character arcs",
        "outputFormat": "Character exploration documents:\n- Deep psychological profiles\n- Backstory narratives\n- Relationship dynamics\n- Character voice samples\n- Visual/physical descriptions\n- Arc progression notes",
        "baseCodeTemplate": "default",
        "lmStudioUrl": "http://localhost:1234/",
        "lmStudioConnectionId": "conn_1749362776"
      },
      {
        "id": "worker-1749360390640",
        "type": "worker",
        "label": "Asset Compilation",
        "position": {
          "x": 3675.060704286283,
          "y": 779.4438172551586
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "tasks": [
          {
            "id": "task-1749360390640",
            "text": "Create unified registry of all story elements with unique identifiers",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360471939",
            "text": "Map element appearances across scenes/beats for production planning",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360472126",
            "text": "Identify special requirements (VFX, props, wardrobe, etc.)",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360472312",
            "text": "Generate dependency matrix showing element relationships",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          }
        ],
        "connectedTo": [
          "worker-1749360498922"
        ],
        "connectedFrom": [
          "worker-1749360338200"
        ],
        "code": "# ========================================================================\r\n# BASE CODE - ê³µí†µ ì‹¤í–‰ ì½”ë“œ (ìˆ˜ì • ë¶ˆê°€)\r\n# ì´ ì½”ë“œëŠ” ëª¨ë“  Worker ë…¸ë“œê°€ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ ì‹¤í–‰ ì½”ë“œì…ë‹ˆë‹¤.\r\n# ========================================================================\r\n\r\nimport json\r\nimport time\r\n\r\n# ì—°ê²°ëœ ì…ë ¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\r\ninput_data = get_connected_outputs()\r\nprint(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Connected inputs received\")\r\nprint(json.dumps(input_data, ensure_ascii=False, indent=2))\r\n\r\n# í˜„ì¬ ë…¸ë“œ ì •ë³´ ì¶œë ¥\r\nprint(f\"\\nNode: {current_node.get('label', 'Unknown')}\")\r\nprint(f\"Purpose: {node_purpose}\")\r\nprint(f\"Expected Output Format: {output_format_description}\")\r\n\r\n# AI ëª¨ë¸ ì„¤ì • í™•ì¸ - execution.pyì—ì„œ ë…¸ë“œ ì„¤ì •ìœ¼ë¡œë¶€í„° ì œê³µë¨\r\nprint(f\"\\n[DEBUG] model_name: {model_name}\")\r\nprint(f\"[DEBUG] lm_studio_url: {lm_studio_url}\")\r\nprint(f\"[DEBUG] current_node['model']: {current_node.get('model', 'Not found')}\")\r\nprint(f\"[DEBUG] current_node['lmStudioUrl']: {current_node.get('lmStudioUrl', 'Not found')}\")\r\n\r\nif model_name == 'none' or not model_name or not lm_studio_url:\r\n    print(\"\\nâš ï¸  No AI model configured!\")\r\n    output = {\r\n        \"error\": \"No AI model configured\",\r\n        \"hint\": \"Please connect to LM Studio and select a model\",\r\n        \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S')\r\n    }\r\nelse:\r\n    print(f\"\\nâœ… Using AI model: {model_name}\")\r\n    \r\n    # ========================================================================\r\n    # ì…ë ¥ ë°ì´í„° í†µí•©\r\n    # ========================================================================\r\n    combined_input = \"\"\r\n    for key, value in input_data.items():\r\n        if isinstance(value, dict):\r\n            if 'text' in value:\r\n                combined_input += f\"[{key}]\\n{value['text']}\\n\\n\"\r\n            elif 'content' in value:\r\n                combined_input += f\"[{key}]\\n{value['content']}\\n\\n\"\r\n            else:\r\n                combined_input += f\"[{key}]\\n{json.dumps(value, ensure_ascii=False, indent=2)}\\n\\n\"\r\n        elif isinstance(value, str):\r\n            combined_input += f\"[{key}]\\n{value}\\n\\n\"\r\n        else:\r\n            combined_input += f\"[{key}]\\n{str(value)}\\n\\n\"\r\n    \r\n    # ========================================================================\r\n    # AI í”„ë¡¬í”„íŠ¸ êµ¬ì„±\r\n    # ========================================================================\r\n    base_prompt = f\"\"\"ë‹¹ì‹ ì€ ë‹¤ìŒ ëª©ì ì„ ë‹¬ì„±í•´ì•¼ í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤:\r\n\r\n**ëª©ì  (Purpose):**\r\n{node_purpose}\r\n\r\n**ì…ë ¥ ë°ì´í„°:**\r\n{combined_input.strip()}\r\n\r\n**ìˆ˜í–‰í•  ì‘ì—…ë“¤ (Tasks):**\"\"\"\r\n    \r\n    # Tasks ì¶”ê°€\r\n    if 'tasks' in current_node and current_node['tasks']:\r\n        for i, task in enumerate(current_node['tasks'], 1):\r\n            base_prompt += f\"\\n{i}. {task['text']}\"\r\n            # Task statusì— ë”°ë¥¸ ì¶”ê°€ ì§€ì‹œ\r\n            if task.get('taskStatus') == 'locked':\r\n                base_prompt += \" [í•„ìˆ˜ - ë°˜ë“œì‹œ ìˆ˜í–‰]\"\r\n            elif task.get('taskStatus') == 'low_priority':\r\n                base_prompt += \" [ì„ íƒì  - ê°€ëŠ¥í•œ ê²½ìš° ìˆ˜í–‰]\"\r\n    else:\r\n        base_prompt += \"\\n(ì‘ì—…ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)\"\r\n    \r\n    base_prompt += f\"\"\"\\n\\n**ê¸°ëŒ€í•˜ëŠ” ì¶œë ¥ í˜•ì‹:**\r\n{output_format_description}\r\n\r\nìœ„ì˜ ëª©ì ê³¼ ì‘ì—…ë“¤ì„ ìˆ˜í–‰í•˜ê³ , ì§€ì •ëœ ì¶œë ¥ í˜•ì‹ì— ë§ì¶° ê²°ê³¼ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\r\n\"\"\"\r\n\r\n    # ========================================================================\r\n    # Experimental Code ë³‘í•© (ìˆëŠ” ê²½ìš°)\r\n    # ========================================================================\r\n    # EXP_CODE_MERGE_POINT - ì´ ë¶€ë¶„ì—ì„œ Exp Codeê°€ ë³‘í•©ë©ë‹ˆë‹¤\r\n    \r\n    # ========================================================================\r\n    # AI ëª¨ë¸ í˜¸ì¶œ\r\n    # ========================================================================\r\n    try:\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] Sending request to AI model...\")\r\n        print(f\"Prompt length: {len(base_prompt)} characters\")\r\n        \r\n        # AI ì‘ë‹µ ë°›ê¸°\r\n        ai_response = call_ai_model(base_prompt)\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] AI response received\")\r\n        \r\n        # ì‘ë‹µ ì²˜ë¦¬\r\n        if isinstance(ai_response, dict) and 'error' in ai_response:\r\n            output = ai_response\r\n        elif isinstance(ai_response, str):\r\n            # JSON ì¶”ì¶œ ì‹œë„\r\n            json_start = ai_response.find('{')\r\n            json_end = ai_response.rfind('}') + 1\r\n            \r\n            if json_start != -1 and json_end > json_start:\r\n                try:\r\n                    output = json.loads(ai_response[json_start:json_end])\r\n                except json.JSONDecodeError:\r\n                    output = {\r\n                        \"result\": ai_response,\r\n                        \"type\": \"text\",\r\n                        \"raw_response\": True\r\n                    }\r\n            else:\r\n                output = {\r\n                    \"result\": ai_response,\r\n                    \"type\": \"text\"\r\n                }\r\n        else:\r\n            output = ai_response\r\n            \r\n        # ë©”íƒ€ë°ì´í„° ì¶”ê°€\r\n        if isinstance(output, dict):\r\n            output['_metadata'] = {\r\n                'model': model_name,\r\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\r\n                'node': current_node.get('label', 'Unknown')\r\n            }\r\n            \r\n    except Exception as e:\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] âŒ Error: {str(e)}\")\r\n        output = {\r\n            \"error\": f\"AI processing failed: {str(e)}\",\r\n            \"type\": \"error\",\r\n            \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S')\r\n        }\r\n\r\n# ========================================================================\r\n# ìµœì¢… ì¶œë ¥\r\n# ========================================================================\r\nprint(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] Final output:\")\r\nprint(json.dumps(output, ensure_ascii=False, indent=2))\r\nprint(f\"\\nâœ… Execution completed successfully\")\r\n\r\n# output ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì—ˆìŒì„ í™•ì¸\r\nprint(f\"\\n[DEBUG] Output is set: {'output' in locals()}\")\r\nprint(f\"[DEBUG] Output value type: {type(output)}\")\r\nprint(f\"[DEBUG] Output is None: {output is None}\")",
        "output": "",
        "model": "qwen2.5-7b-instruct-uncensored",
        "purpose": "Consolidate all story elements into organized production-ready asset registry with cross-references",
        "outputFormat": "Comprehensive asset manifest:\n- Element catalog with IDs\n- Cross-reference matrix\n- Scene appearance tracking\n- Production requirements\n- Dependency mappings",
        "baseCodeTemplate": "default",
        "lmStudioUrl": "http://localhost:1234/",
        "lmStudioConnectionId": "conn_1749362787"
      },
      {
        "id": "worker-1749360498922",
        "type": "worker",
        "label": "Scene Breakdown",
        "position": {
          "x": 4418.640064581576,
          "y": 823.4333743507012
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "tasks": [
          {
            "id": "task-1749360498922",
            "text": "Transform story beats into individual scenes with clear boundaries",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360516561",
            "text": "Write detailed action descriptions with visual storytelling in mind",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360533910",
            "text": "Include camera angles, movements, and shot compositions",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          },
          {
            "id": "task-1749360537296",
            "text": "Estimate scene duration and pacing rhythm",
            "status": "pending",
            "taskStatus": "editable",
            "aiScore": 50.0
          }
        ],
        "connectedTo": [
          "output-preproduction-script"
        ],
        "connectedFrom": [
          "worker-1749360390640"
        ],
        "code": "# ========================================================================\r\n# BASE CODE - ê³µí†µ ì‹¤í–‰ ì½”ë“œ (ìˆ˜ì • ë¶ˆê°€)\r\n# ì´ ì½”ë“œëŠ” ëª¨ë“  Worker ë…¸ë“œê°€ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ ì‹¤í–‰ ì½”ë“œì…ë‹ˆë‹¤.\r\n# ========================================================================\r\n\r\nimport json\r\nimport time\r\n\r\n# ì—°ê²°ëœ ì…ë ¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\r\ninput_data = get_connected_outputs()\r\nprint(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Connected inputs received\")\r\nprint(json.dumps(input_data, ensure_ascii=False, indent=2))\r\n\r\n# í˜„ì¬ ë…¸ë“œ ì •ë³´ ì¶œë ¥\r\nprint(f\"\\nNode: {current_node.get('label', 'Unknown')}\")\r\nprint(f\"Purpose: {node_purpose}\")\r\nprint(f\"Expected Output Format: {output_format_description}\")\r\n\r\n# AI ëª¨ë¸ ì„¤ì • í™•ì¸ - execution.pyì—ì„œ ë…¸ë“œ ì„¤ì •ìœ¼ë¡œë¶€í„° ì œê³µë¨\r\nprint(f\"\\n[DEBUG] model_name: {model_name}\")\r\nprint(f\"[DEBUG] lm_studio_url: {lm_studio_url}\")\r\nprint(f\"[DEBUG] current_node['model']: {current_node.get('model', 'Not found')}\")\r\nprint(f\"[DEBUG] current_node['lmStudioUrl']: {current_node.get('lmStudioUrl', 'Not found')}\")\r\n\r\nif model_name == 'none' or not model_name or not lm_studio_url:\r\n    print(\"\\nâš ï¸  No AI model configured!\")\r\n    output = {\r\n        \"error\": \"No AI model configured\",\r\n        \"hint\": \"Please connect to LM Studio and select a model\",\r\n        \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S')\r\n    }\r\nelse:\r\n    print(f\"\\nâœ… Using AI model: {model_name}\")\r\n    \r\n    # ========================================================================\r\n    # ì…ë ¥ ë°ì´í„° í†µí•©\r\n    # ========================================================================\r\n    combined_input = \"\"\r\n    for key, value in input_data.items():\r\n        if isinstance(value, dict):\r\n            if 'text' in value:\r\n                combined_input += f\"[{key}]\\n{value['text']}\\n\\n\"\r\n            elif 'content' in value:\r\n                combined_input += f\"[{key}]\\n{value['content']}\\n\\n\"\r\n            else:\r\n                combined_input += f\"[{key}]\\n{json.dumps(value, ensure_ascii=False, indent=2)}\\n\\n\"\r\n        elif isinstance(value, str):\r\n            combined_input += f\"[{key}]\\n{value}\\n\\n\"\r\n        else:\r\n            combined_input += f\"[{key}]\\n{str(value)}\\n\\n\"\r\n    \r\n    # ========================================================================\r\n    # AI í”„ë¡¬í”„íŠ¸ êµ¬ì„±\r\n    # ========================================================================\r\n    base_prompt = f\"\"\"ë‹¹ì‹ ì€ ë‹¤ìŒ ëª©ì ì„ ë‹¬ì„±í•´ì•¼ í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤:\r\n\r\n**ëª©ì  (Purpose):**\r\n{node_purpose}\r\n\r\n**ì…ë ¥ ë°ì´í„°:**\r\n{combined_input.strip()}\r\n\r\n**ìˆ˜í–‰í•  ì‘ì—…ë“¤ (Tasks):**\"\"\"\r\n    \r\n    # Tasks ì¶”ê°€\r\n    if 'tasks' in current_node and current_node['tasks']:\r\n        for i, task in enumerate(current_node['tasks'], 1):\r\n            base_prompt += f\"\\n{i}. {task['text']}\"\r\n            # Task statusì— ë”°ë¥¸ ì¶”ê°€ ì§€ì‹œ\r\n            if task.get('taskStatus') == 'locked':\r\n                base_prompt += \" [í•„ìˆ˜ - ë°˜ë“œì‹œ ìˆ˜í–‰]\"\r\n            elif task.get('taskStatus') == 'low_priority':\r\n                base_prompt += \" [ì„ íƒì  - ê°€ëŠ¥í•œ ê²½ìš° ìˆ˜í–‰]\"\r\n    else:\r\n        base_prompt += \"\\n(ì‘ì—…ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)\"\r\n    \r\n    base_prompt += f\"\"\"\\n\\n**ê¸°ëŒ€í•˜ëŠ” ì¶œë ¥ í˜•ì‹:**\r\n{output_format_description}\r\n\r\nìœ„ì˜ ëª©ì ê³¼ ì‘ì—…ë“¤ì„ ìˆ˜í–‰í•˜ê³ , ì§€ì •ëœ ì¶œë ¥ í˜•ì‹ì— ë§ì¶° ê²°ê³¼ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\r\n\"\"\"\r\n\r\n    # ========================================================================\r\n    # Experimental Code ë³‘í•© (ìˆëŠ” ê²½ìš°)\r\n    # ========================================================================\r\n    # EXP_CODE_MERGE_POINT - ì´ ë¶€ë¶„ì—ì„œ Exp Codeê°€ ë³‘í•©ë©ë‹ˆë‹¤\r\n    \r\n    # ========================================================================\r\n    # AI ëª¨ë¸ í˜¸ì¶œ\r\n    # ========================================================================\r\n    try:\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] Sending request to AI model...\")\r\n        print(f\"Prompt length: {len(base_prompt)} characters\")\r\n        \r\n        # AI ì‘ë‹µ ë°›ê¸°\r\n        ai_response = call_ai_model(base_prompt)\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] AI response received\")\r\n        \r\n        # ì‘ë‹µ ì²˜ë¦¬\r\n        if isinstance(ai_response, dict) and 'error' in ai_response:\r\n            output = ai_response\r\n        elif isinstance(ai_response, str):\r\n            # JSON ì¶”ì¶œ ì‹œë„\r\n            json_start = ai_response.find('{')\r\n            json_end = ai_response.rfind('}') + 1\r\n            \r\n            if json_start != -1 and json_end > json_start:\r\n                try:\r\n                    output = json.loads(ai_response[json_start:json_end])\r\n                except json.JSONDecodeError:\r\n                    output = {\r\n                        \"result\": ai_response,\r\n                        \"type\": \"text\",\r\n                        \"raw_response\": True\r\n                    }\r\n            else:\r\n                output = {\r\n                    \"result\": ai_response,\r\n                    \"type\": \"text\"\r\n                }\r\n        else:\r\n            output = ai_response\r\n            \r\n        # ë©”íƒ€ë°ì´í„° ì¶”ê°€\r\n        if isinstance(output, dict):\r\n            output['_metadata'] = {\r\n                'model': model_name,\r\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\r\n                'node': current_node.get('label', 'Unknown')\r\n            }\r\n            \r\n    except Exception as e:\r\n        print(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] âŒ Error: {str(e)}\")\r\n        output = {\r\n            \"error\": f\"AI processing failed: {str(e)}\",\r\n            \"type\": \"error\",\r\n            \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S')\r\n        }\r\n\r\n# ========================================================================\r\n# ìµœì¢… ì¶œë ¥\r\n# ========================================================================\r\nprint(f\"\\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] Final output:\")\r\nprint(json.dumps(output, ensure_ascii=False, indent=2))\r\nprint(f\"\\nâœ… Execution completed successfully\")\r\n\r\n# output ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì—ˆìŒì„ í™•ì¸\r\nprint(f\"\\n[DEBUG] Output is set: {'output' in locals()}\")\r\nprint(f\"[DEBUG] Output value type: {type(output)}\")\r\nprint(f\"[DEBUG] Output is None: {output is None}\")",
        "output": "",
        "model": "sakura-13b-korean-v0.9",
        "purpose": "Generate detailed scene-by-scene script with visual directions, timing, and technical annotations",
        "outputFormat": "Cinematic script format:\n- Scene headers with location/time\n- Action descriptions\n- Camera directions\n- Dialogue (if any)\n- Timing estimates\n- Technical notes",
        "baseCodeTemplate": "default",
        "lmStudioUrl": "http://localhost:1234/",
        "lmStudioConnectionId": "conn_1749362804"
      }
    ],
    "inputConfig": {
      "sources": [],
      "selectedItems": [],
      "projectId": "bb03d6f6-8bc0-4ea0-b56a-8b5e93ee5441"
    }
  },
  "preproduction-storyboard": {
    "id": "preproduction-storyboard",
    "name": "Storyboard",
    "group": "preproduction",
    "nodes": [
      {
        "id": "input-preproduction-storyboard",
        "type": "input",
        "label": "Input",
        "position": {
          "x": 100.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      },
      {
        "id": "output-preproduction-storyboard",
        "type": "output",
        "label": "Output",
        "position": {
          "x": 700.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      }
    ]
  },
  "preproduction-planning": {
    "id": "preproduction-planning",
    "name": "Planning",
    "group": "preproduction",
    "nodes": [
      {
        "id": "input-preproduction-planning",
        "type": "input",
        "label": "Input",
        "position": {
          "x": 100.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      },
      {
        "id": "output-preproduction-planning",
        "type": "output",
        "label": "Output",
        "position": {
          "x": 700.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      }
    ]
  },
  "postproduction-modeling": {
    "id": "postproduction-modeling",
    "name": "Modeling",
    "group": "postproduction",
    "nodes": [
      {
        "id": "input-postproduction-modeling",
        "type": "input",
        "label": "Input",
        "position": {
          "x": 100.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      },
      {
        "id": "output-postproduction-modeling",
        "type": "output",
        "label": "Output",
        "position": {
          "x": 700.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      }
    ]
  },
  "postproduction-rigging": {
    "id": "postproduction-rigging",
    "name": "Rigging",
    "group": "postproduction",
    "nodes": [
      {
        "id": "input-postproduction-rigging",
        "type": "input",
        "label": "Input",
        "position": {
          "x": 100.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      },
      {
        "id": "output-postproduction-rigging",
        "type": "output",
        "label": "Output",
        "position": {
          "x": 700.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      }
    ]
  },
  "postproduction-texture": {
    "id": "postproduction-texture",
    "name": "Texture",
    "group": "postproduction",
    "nodes": [
      {
        "id": "input-postproduction-texture",
        "type": "input",
        "label": "Input",
        "position": {
          "x": 100.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      },
      {
        "id": "output-postproduction-texture",
        "type": "output",
        "label": "Output",
        "position": {
          "x": 700.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      }
    ]
  },
  "postproduction-animation": {
    "id": "postproduction-animation",
    "name": "Animation",
    "group": "postproduction",
    "nodes": [
      {
        "id": "input-postproduction-animation",
        "type": "input",
        "label": "Input",
        "position": {
          "x": 100.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      },
      {
        "id": "output-postproduction-animation",
        "type": "output",
        "label": "Output",
        "position": {
          "x": 700.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      }
    ]
  },
  "postproduction-vfx": {
    "id": "postproduction-vfx",
    "name": "VFX",
    "group": "postproduction",
    "nodes": [
      {
        "id": "input-postproduction-vfx",
        "type": "input",
        "label": "Input",
        "position": {
          "x": 100.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      },
      {
        "id": "output-postproduction-vfx",
        "type": "output",
        "label": "Output",
        "position": {
          "x": 700.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      }
    ]
  },
  "postproduction-lighting-&-rendering": {
    "id": "postproduction-lighting-&-rendering",
    "name": "Lighting & Rendering",
    "group": "postproduction",
    "nodes": [
      {
        "id": "input-postproduction-lighting-&-rendering",
        "type": "input",
        "label": "Input",
        "position": {
          "x": 100.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      },
      {
        "id": "output-postproduction-lighting-&-rendering",
        "type": "output",
        "label": "Output",
        "position": {
          "x": 700.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      }
    ]
  },
  "postproduction-sound-design": {
    "id": "postproduction-sound-design",
    "name": "Sound Design",
    "group": "postproduction",
    "nodes": [
      {
        "id": "input-postproduction-sound-design",
        "type": "input",
        "label": "Input",
        "position": {
          "x": 100.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      },
      {
        "id": "output-postproduction-sound-design",
        "type": "output",
        "label": "Output",
        "position": {
          "x": 700.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      }
    ]
  },
  "postproduction-compositing": {
    "id": "postproduction-compositing",
    "name": "Compositing",
    "group": "postproduction",
    "nodes": [
      {
        "id": "input-postproduction-compositing",
        "type": "input",
        "label": "Input",
        "position": {
          "x": 100.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      },
      {
        "id": "output-postproduction-compositing",
        "type": "output",
        "label": "Output",
        "position": {
          "x": 700.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      }
    ]
  },
  "director-direction": {
    "id": "director-direction",
    "name": "Direction",
    "group": "director",
    "nodes": [
      {
        "id": "input-director-direction",
        "type": "input",
        "label": "Input",
        "position": {
          "x": 100.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      },
      {
        "id": "output-director-direction",
        "type": "output",
        "label": "Output",
        "position": {
          "x": 700.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      }
    ]
  },
  "director-review": {
    "id": "director-review",
    "name": "Review",
    "group": "director",
    "nodes": [
      {
        "id": "input-director-review",
        "type": "input",
        "label": "Input",
        "position": {
          "x": 100.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      },
      {
        "id": "output-director-review",
        "type": "output",
        "label": "Output",
        "position": {
          "x": 700.0,
          "y": 200.0
        },
        "isRunning": false,
        "isDeactivated": false,
        "supervised": false,
        "connectedFrom": [],
        "code": "",
        "output": "",
        "purpose": "",
        "outputFormat": "",
        "baseCodeTemplate": "default"
      }
    ]
  }
}